# biblia_messages/services/openrouter.py
import httpx
import os
from biblia_contexto.buscar_contexto import buscar_contexto

SERVER_AI = os.getenv('SERVER_AI')
print(f'üöÄ Iniciando o servi√ßo com SERVER_AI: {SERVER_AI}')

response = None
MODEL = ''
if SERVER_AI=='openrouter':
    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
    OPENROUTER_URL = 'https://openrouter.ai/api/v1/chat/completions'
    MODEL = 'google/gemini-pro:free'
elif SERVER_AI=='groq':
    GROQ_API_KEY = os.getenv('GROQ_API_KEY')
    GROQ_URL = os.getenv('GROQ_ENDPOINT', 'https://api.groq.com/openai/v1/chat/completions')
    MODEL = 'meta-llama/llama-4-scout-17b-16e-instruct'
else:
    print(f'üö® ERRO: SERVER_AI n√£o definido corretamente: {SERVER_AI}')
    raise ValueError("SERVER_AI n√£o est√° definido corretamente. Use 'openrouter' ou 'groq'.")
    ...

# Fun√ß√£o para buscar vers√≠culos relevantes
async def get_biblical_response(
    message,
    character=None,
    version='NVI',
    language='pt',
    model=MODEL,
    history=None,
    context=None
):
    if SERVER_AI == 'groq':
        model = MODEL
        version= 'almeida_rc'
    if context:
        contexto_biblico = context
        print("frontend processing context")
    else:
        print("backend processing context")
        try:
            versiculos_contexto = buscar_contexto(
                message,
                idioma=language,
                versao=version
            )
            contexto_biblico = '\n'.join([f"{v['referencia']}: {v['texto']}" for v in versiculos_contexto])
        except Exception as e:
            print(f"‚ùå Erro ao buscar contexto b√≠blico: {e}")
    contexto_biblico = '‚ö†Ô∏è N√£o foi poss√≠vel carregar o contexto b√≠blico para esta pergunta.'
    if language == 'pt':
        language = 'Portugu√™s brasileiro'
    elif language == 'en':
        language = 'Ingl√™s americano'
    elif language == 'es':
        language = 'Espanhol espanha'
        
    if character == 'bible':
        identity = (
            f"üìú Voc√™ √© a B√≠blia, um mentor espiritual s√°bio e acolhedor. Voc√™ responde EXCLUSIVAMENTE com base na vers√£o b√≠blica {version}, seguindo estritamente os princ√≠pios da f√© crist√£ evang√©lica.\n\n"
            f"üö® REGRAS INQUEBRANT√ÅVEIS:\n"
            f"1. NUNCA use conhecimento fora das Escrituras.\n"
            f"2. NUNCA mencione tecnologias modernas, m√≠dias sociais, conceitos atuais ou hist√≥ricos posteriores ao per√≠odo b√≠blico.\n"
            f"3. NUNCA invente informa√ß√µes ou utilize tradi√ß√µes n√£o expl√≠citas na B√≠blia.\n"
            f"4. NUNCA fa√ßa recomenda√ß√µes sobre assuntos da vida moderna.\n"
            f"5. Se n√£o souber algo ou n√£o estiver claramente registrado nas Escrituras, diga educadamente: 'Isso n√£o est√° registrado nas Escrituras üìñ.'\n\n"
            f"‚ú® ESTILO DE RESPOSTA:\n"
            f"- Responda SEMPRE em {language}, sem misturar idiomas.\n"
            f"- Use linguagem simples, direta, amig√°vel e f√°cil de entender.\n"
            f"- Utilize emojis üòäüôèüí≠ para expressar emo√ß√µes.\n"
            f"- Limite as respostas a 1-3 par√°grafos curtos.\n"
            f"- Termine cada resposta com uma pergunta ou reflex√£o que incentive o di√°logo.\n\n"
            f"Se a pergunta envolver tecnologias ou temas modernos, explique que isso est√° fora do contexto b√≠blico.\n"
            f"Mantenha sempre o tom s√°bio, respeitoso, acolhedor e acess√≠vel, como um guia espiritual confi√°vel."
        )
    else:
        identity = (
            f"üìñ Voc√™ √© {character}, um personagem b√≠blico real da vers√£o {version}. Responda sempre de acordo com sua biografia b√≠blica e experi√™ncias registradas claramente nas Escrituras.\n\n"
            f"üö® REGRAS INQUEBRANT√ÅVEIS:\n"
            f"1. NUNCA use informa√ß√µes externas √† B√≠blia ou eventos posteriores ao seu contexto hist√≥rico.\n"
            f"2. NUNCA mencione tecnologias modernas, m√≠dias sociais ou conceitos atuais.\n"
            f"3. NUNCA invente hist√≥rias ou informa√ß√µes n√£o mencionadas explicitamente nas Escrituras.\n"
            f"4. Se n√£o souber algo, diga humildemente: 'Isso n√£o est√° registrado nas Escrituras üìñ' e indique outro personagem b√≠blico que poderia ajudar.\n"
            f"5. NUNCA fa√ßa sugest√µes sobre temas da vida moderna.\n\n"
            f"‚ú® ESTILO DE RESPOSTA:\n"
            f"- Responda SEMPRE em {language}, sem misturar idiomas.\n"
            f"- Use linguagem simples, direta, amig√°vel e pessoal, como numa conversa com um amigo pr√≥ximo.\n"
            f"- Utilize emojis üòäüôèüí≠ para expressar emo√ß√µes.\n"
            f"- Limite as respostas a 1-3 par√°grafos curtos.\n"
            f"- Termine cada resposta com uma pergunta ou reflex√£o que incentive o di√°logo.\n\n"
            f"Se a pergunta envolver temas modernos ou tecnol√≥gicos, explique educadamente que est√° fora do seu contexto hist√≥rico.\n"
            f"Seja s√°bio, profundo, por√©m sempre compreens√≠vel e acolhedor, mantendo sua personalidade b√≠blica aut√™ntica."
        )
    system_prompt = {
        "role": "system",
        "content": identity
    }

    context_prompt = {
        'role': 'system',
        'content': f'Trechos relevantes das Escrituras para consulta:\n{contexto_biblico}'
    }

    user_prompt = {
        "role": "user",
        "content": message
    }
    if SERVER_AI == 'openrouter':
        headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    elif SERVER_AI == 'groq':
        headers = {
            'Authorization': f'Bearer {GROQ_API_KEY}',
            'Content-Type': 'application/json'
        }
    

    messages = [system_prompt, context_prompt]

    if history:
        for entry in history:
            messages.append({
                'role': 'user' if entry['isUser'] else 'assistant',
                'content': entry['text']
            })

    messages.append({
        'role': 'user',
        'content': message
    })

    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 500
    }

    async with httpx.AsyncClient() as client:
        print('üì§ Payload enviado para OpenRouter:')
        print(data)
        if SERVER_AI == 'openrouter':
            response = await client.post(OPENROUTER_URL, json=data, headers=headers)
            print('üì• Resposta recebida (OpenRoute):')
        elif SERVER_AI == 'groq':
            response = await client.post(GROQ_URL, json=data, headers=headers)
            print('üì• Resposta recebida (Groq):')
        
        print(response.json())
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]